{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT\n",
    "\n",
    "Paper - https://arxiv.org/pdf/1910.01108.pdf\n",
    "\n",
    "Blog - https://medium.com/huggingface/distilbert-8cf3380435b5\n",
    "    \n",
    "### Differences with BERT\n",
    "\n",
    "* Number of layers is reduced by a factor of 2\n",
    "* Dynamic word masking similar to RoBERTa\n",
    "* NSP (Next Sentence Prediction) task is removed similar to RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1106 05:10:38.340166 140508741437184 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1106 05:10:38.374237 140508741437184 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from transformers import (WEIGHTS_NAME, \n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer, \n",
    "                          XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"../input/ClothingReviews.csv\")\n",
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.3, random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_features(examples, tokenizer,\n",
    "                                      max_length=512,\n",
    "                                      pad_on_left=False,\n",
    "                                      pad_token=0,\n",
    "                                      pad_token_segment_id=0,\n",
    "                                      mask_padding_with_zero=True):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of ``InputFeatures``\n",
    "    Args:\n",
    "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        max_length: Maximum example length\n",
    "        task: GLUE task\n",
    "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
    "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
    "        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
    "        pad_token: Padding token\n",
    "        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n",
    "        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n",
    "            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n",
    "            actual values)\n",
    "    Returns:\n",
    "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
    "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
    "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
    "    \"\"\"\n",
    "    features = [[],[],[]]\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "#         if ex_index % 10000 == 0:\n",
    "#             print(\"Writing example %d\" % (ex_index))\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            example,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
    "            token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
    "        else:\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
    "        assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n",
    "        assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n",
    "\n",
    "#         if ex_index < 1:\n",
    "#             print(\"*** Example ***\")\n",
    "#             print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             print(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "#             print(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "\n",
    "        features[0].append(input_ids)\n",
    "        features[1].append(attention_mask)\n",
    "        features[2].append(token_type_ids)\n",
    "\n",
    "    return features\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1106 05:10:41.544731 140508741437184 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpm10ez5qs\n",
      "100%|██████████| 492/492 [00:00<00:00, 124952.93B/s]\n",
      "I1106 05:10:41.913883 140508741437184 file_utils.py:309] copying /tmp/tmpm10ez5qs to cache at /home/srk/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1106 05:10:41.917099 140508741437184 file_utils.py:313] creating metadata file for /home/srk/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1106 05:10:41.919893 140508741437184 file_utils.py:322] removing temp file /tmp/tmpm10ez5qs\n",
      "I1106 05:10:41.921376 140508741437184 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /home/srk/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1106 05:10:41.923851 140508741437184 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1106 05:10:42.269354 140508741437184 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/srk/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1106 05:10:42.654695 140508741437184 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /home/srk/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1106 05:10:42.658258 140508741437184 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1106 05:10:43.004731 140508741437184 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp8acvte33\n",
      "100%|██████████| 267967963/267967963 [00:13<00:00, 19204458.65B/s]\n",
      "I1106 05:10:57.399789 140508741437184 file_utils.py:309] copying /tmp/tmp8acvte33 to cache at /home/srk/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "I1106 05:10:57.743621 140508741437184 file_utils.py:313] creating metadata file for /home/srk/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "I1106 05:10:57.745541 140508741437184 file_utils.py:322] removing temp file /tmp/tmp8acvte33\n",
      "I1106 05:10:57.806261 140508741437184 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at /home/srk/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "I1106 05:10:59.600623 140508741437184 modeling_utils.py:405] Weights of DistilBertForSequenceClassification not initialized from pretrained model: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "I1106 05:10:59.602108 140508741437184 modeling_utils.py:408] Weights from pretrained model not used in DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "/home/srk/env/DS2/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model_start_time = time.time()\n",
    "# Model Config and initialize\n",
    "model_name = \"distilbert\"\n",
    "pretrained_model_name = \"distilbert-base-uncased\"\n",
    "n_classes = 1\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[model_name]\n",
    "config = config_class.from_pretrained(pretrained_model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_model_name, do_lower_case=True)\n",
    "model = model_class.from_pretrained(pretrained_model_name, num_labels=1)\n",
    "\n",
    "# Dataset Preparation\n",
    "max_length = 128\n",
    "train_df[\"text\"] = train_df[\"text\"].astype(str).fillna(\"NA\")\n",
    "train_features = convert_text_to_features(train_df[\"text\"], tokenizer, max_length=max_length)\n",
    "X = torch.tensor(train_features[0], dtype=torch.long)\n",
    "X_mask = torch.tensor(train_features[1], dtype=torch.long)\n",
    "X_seg_ids = torch.tensor(train_features[2], dtype=torch.long)\n",
    "y = train_df[\"recommended\"].values\n",
    "y = torch.tensor(y[:,np.newaxis], dtype=torch.float32)\n",
    "\n",
    "batch_size = 8\n",
    "train_dataset = data.TensorDataset(X, X_mask, X_seg_ids, y)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model Building\n",
    "n_epochs = 1\n",
    "accumulation_steps = 1\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "num_train_optimization_steps = int(n_epochs*len(train_dataset)/batch_size/accumulation_steps)\n",
    "num_warmup_steps = int(0.05*num_train_optimization_steps)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
    "scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                 warmup_steps=num_warmup_steps,\n",
    "                                 t_total=num_train_optimization_steps)\n",
    "\n",
    "seed_everything()\n",
    "model.to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for x_batch, x_mask, x_seg_ids, y_batch in train_loader:\n",
    "        outputs = model(x_batch.to(device),\n",
    "                        attention_mask=x_mask.to(device),\n",
    "#                         token_type_ids=x_seg_ids.to(device),\n",
    "                        labels=None)\n",
    "        y_pred = outputs[0]\n",
    "        loss = loss_fn(y_pred, y_batch.to(device))\n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "model_end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 10.7 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srk/env/DS2/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9572450487732781\n"
     ]
    }
   ],
   "source": [
    "pred_start_time = time.time()\n",
    "# Data Preparation\n",
    "test_df[\"text\"] = test_df[\"text\"].astype(str).fillna(\"NA\")\n",
    "test_features = convert_text_to_features(test_df[\"text\"], tokenizer, max_length=max_length)\n",
    "\n",
    "test_X = torch.tensor(test_features[0], dtype=torch.long)\n",
    "test_X_mask = torch.tensor(test_features[1], dtype=torch.long)\n",
    "test_X_seg_ids = torch.tensor(test_features[2], dtype=torch.long)\n",
    "test_y = test_df[\"recommended\"].values\n",
    "\n",
    "test_dataset = data.TensorDataset(test_X, test_X_mask, test_X_seg_ids)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Prediction\n",
    "preds = np.zeros([len(test_dataset), 1])\n",
    "model.eval()\n",
    "for i, (x_batch, x_mask, x_seg_ids) in enumerate(test_loader):\n",
    "    outputs = model(x_batch.to(device),\n",
    "                    attention_mask=x_mask.to(device),\n",
    "#                     token_type_ids=x_seg_ids.to(device),\n",
    "                    labels=None)\n",
    "    y_pred = sigmoid(outputs[0].detach().cpu().numpy())\n",
    "    preds[i*batch_size:(i+1)*batch_size, :] = y_pred\n",
    "    \n",
    "from sklearn import metrics\n",
    "print(metrics.roc_auc_score(test_y, preds))\n",
    "pred_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99616832],\n",
       "       [0.996566  ],\n",
       "       [0.98557997],\n",
       "       ...,\n",
       "       [0.97914416],\n",
       "       [0.99113232],\n",
       "       [0.96795791]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.66015911102295\n",
      "23.971781253814697\n"
     ]
    }
   ],
   "source": [
    "print(model_end_time - model_start_time)\n",
    "print(pred_end_time - pred_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
