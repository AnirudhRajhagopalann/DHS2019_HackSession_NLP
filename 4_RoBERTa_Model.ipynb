{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa\n",
    "\n",
    "Paper - https://arxiv.org/pdf/1907.11692.pdf\n",
    "\n",
    "### Differences from BERT\n",
    "\n",
    "* Static Vs Dynamic Masking\n",
    "* NSP (Next Sentence Prediction) objective is removed\n",
    "* Larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1106 05:16:54.155798 139803614635776 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1106 05:16:54.188442 139803614635776 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from transformers import (WEIGHTS_NAME, \n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer, \n",
    "                          XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"../input/ClothingReviews.csv\")\n",
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.3, random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_features(examples, tokenizer,\n",
    "                                      max_length=512,\n",
    "                                      pad_on_left=False,\n",
    "                                      pad_token=0,\n",
    "                                      pad_token_segment_id=0,\n",
    "                                      mask_padding_with_zero=True):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of ``InputFeatures``\n",
    "    Args:\n",
    "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        max_length: Maximum example length\n",
    "        task: GLUE task\n",
    "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
    "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
    "        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
    "        pad_token: Padding token\n",
    "        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n",
    "        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n",
    "            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n",
    "            actual values)\n",
    "    Returns:\n",
    "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
    "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
    "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
    "    \"\"\"\n",
    "    features = [[],[],[]]\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "#         if ex_index % 10000 == 0:\n",
    "#             print(\"Writing example %d\" % (ex_index))\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            example,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
    "            token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
    "        else:\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
    "        assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n",
    "        assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n",
    "\n",
    "#         if ex_index < 1:\n",
    "#             print(\"*** Example ***\")\n",
    "#             print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             print(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "#             print(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "\n",
    "        features[0].append(input_ids)\n",
    "        features[1].append(attention_mask)\n",
    "        features[2].append(token_type_ids)\n",
    "\n",
    "    return features\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1106 05:16:54.680823 139803614635776 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /tmp/tmp8pjgt5ip\n",
      "100%|██████████| 473/473 [00:00<00:00, 121771.78B/s]\n",
      "I1106 05:16:55.041465 139803614635776 file_utils.py:309] copying /tmp/tmp8pjgt5ip to cache at /home/srk/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.9dad9043216064080cf9dd3711c53c0f11fe2b09313eaa66931057b4bdcaf068\n",
      "I1106 05:16:55.044054 139803614635776 file_utils.py:313] creating metadata file for /home/srk/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.9dad9043216064080cf9dd3711c53c0f11fe2b09313eaa66931057b4bdcaf068\n",
      "I1106 05:16:55.046086 139803614635776 file_utils.py:322] removing temp file /tmp/tmp8pjgt5ip\n",
      "I1106 05:16:55.047217 139803614635776 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/srk/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.9dad9043216064080cf9dd3711c53c0f11fe2b09313eaa66931057b4bdcaf068\n",
      "I1106 05:16:55.048627 139803614635776 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I1106 05:16:55.374565 139803614635776 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /tmp/tmpuhjlqdk6\n",
      "100%|██████████| 898823/898823 [00:00<00:00, 2496180.78B/s]\n",
      "I1106 05:16:56.121314 139803614635776 file_utils.py:309] copying /tmp/tmpuhjlqdk6 to cache at /home/srk/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I1106 05:16:56.124668 139803614635776 file_utils.py:313] creating metadata file for /home/srk/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I1106 05:16:56.126111 139803614635776 file_utils.py:322] removing temp file /tmp/tmpuhjlqdk6\n",
      "I1106 05:16:56.470025 139803614635776 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /tmp/tmpofqmvyt0\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 1608449.95B/s]\n",
      "I1106 05:16:57.124030 139803614635776 file_utils.py:309] copying /tmp/tmpofqmvyt0 to cache at /home/srk/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I1106 05:16:57.127424 139803614635776 file_utils.py:313] creating metadata file for /home/srk/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I1106 05:16:57.128802 139803614635776 file_utils.py:322] removing temp file /tmp/tmpofqmvyt0\n",
      "I1106 05:16:57.130086 139803614635776 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/srk/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I1106 05:16:57.131069 139803614635776 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/srk/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I1106 05:16:57.576601 139803614635776 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/srk/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.9dad9043216064080cf9dd3711c53c0f11fe2b09313eaa66931057b4bdcaf068\n",
      "I1106 05:16:57.581501 139803614635776 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I1106 05:16:57.913447 139803614635776 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp2g9t9umb\n",
      "100%|██████████| 501200538/501200538 [00:24<00:00, 20799374.98B/s]\n",
      "I1106 05:17:22.397878 139803614635776 file_utils.py:309] copying /tmp/tmp2g9t9umb to cache at /home/srk/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I1106 05:17:23.029026 139803614635776 file_utils.py:313] creating metadata file for /home/srk/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I1106 05:17:23.031198 139803614635776 file_utils.py:322] removing temp file /tmp/tmp2g9t9umb\n",
      "I1106 05:17:23.142050 139803614635776 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /home/srk/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I1106 05:17:27.809093 139803614635776 modeling_utils.py:405] Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I1106 05:17:27.812370 139803614635776 modeling_utils.py:408] Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "/home/srk/env/DS2/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "model_start_time = time.time()\n",
    "# Model Config and initialize\n",
    "model_name = \"roberta\"\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "n_classes = 1\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[model_name]\n",
    "config = config_class.from_pretrained(pretrained_model_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_model_name, do_lower_case=False)\n",
    "model = model_class.from_pretrained(pretrained_model_name, num_labels=1)\n",
    "\n",
    "# Dataset Preparation\n",
    "max_length = 128\n",
    "train_df[\"text\"] = train_df[\"text\"].astype(str).fillna(\"NA\")\n",
    "train_features = convert_text_to_features(train_df[\"text\"], tokenizer, max_length=max_length)\n",
    "X = torch.tensor(train_features[0], dtype=torch.long)\n",
    "X_mask = torch.tensor(train_features[1], dtype=torch.long)\n",
    "X_seg_ids = torch.tensor(train_features[2], dtype=torch.long)\n",
    "y = train_df[\"recommended\"].values\n",
    "y = torch.tensor(y[:,np.newaxis], dtype=torch.float32)\n",
    "\n",
    "batch_size = 8\n",
    "train_dataset = data.TensorDataset(X, X_mask, X_seg_ids, y)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model Building\n",
    "n_epochs = 1\n",
    "accumulation_steps = 1\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "num_train_optimization_steps = int(n_epochs*len(train_dataset)/batch_size/accumulation_steps)\n",
    "num_warmup_steps = int(0.05*num_train_optimization_steps)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
    "scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                 warmup_steps=num_warmup_steps,\n",
    "                                 t_total=num_train_optimization_steps)\n",
    "\n",
    "seed_everything()\n",
    "model.to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for x_batch, x_mask, x_seg_ids, y_batch in train_loader:\n",
    "        outputs = model(x_batch.to(device),\n",
    "                        attention_mask=x_mask.to(device),\n",
    "                        token_type_ids=x_seg_ids.to(device),\n",
    "                        labels=None)\n",
    "        y_pred = outputs[0]\n",
    "        loss = loss_fn(y_pred, y_batch.to(device))\n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "model_end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srk/env/DS2/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9635614176358467\n"
     ]
    }
   ],
   "source": [
    "pred_start_time = time.time()\n",
    "# Data Preparation\n",
    "test_df[\"text\"] = test_df[\"text\"].astype(str).fillna(\"NA\")\n",
    "test_features = convert_text_to_features(test_df[\"text\"], tokenizer, max_length=max_length)\n",
    "\n",
    "test_X = torch.tensor(test_features[0], dtype=torch.long)\n",
    "test_X_mask = torch.tensor(test_features[1], dtype=torch.long)\n",
    "test_X_seg_ids = torch.tensor(test_features[2], dtype=torch.long)\n",
    "test_y = test_df[\"recommended\"].values\n",
    "\n",
    "test_dataset = data.TensorDataset(test_X, test_X_mask, test_X_seg_ids)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Prediction\n",
    "preds = np.zeros([len(test_dataset), 1])\n",
    "model.eval()\n",
    "for i, (x_batch, x_mask, x_seg_ids) in enumerate(test_loader):\n",
    "    outputs = model(x_batch.to(device),\n",
    "                    attention_mask=x_mask.to(device),\n",
    "                    token_type_ids=x_seg_ids.to(device),\n",
    "                    labels=None)\n",
    "    y_pred = sigmoid(outputs[0].detach().cpu().numpy())\n",
    "    preds[i*batch_size:(i+1)*batch_size, :] = y_pred\n",
    "    \n",
    "from sklearn import metrics\n",
    "print(metrics.roc_auc_score(test_y, preds))\n",
    "pred_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9979741 ],\n",
       "       [0.99806815],\n",
       "       [0.97840422],\n",
       "       ...,\n",
       "       [0.84606194],\n",
       "       [0.9881072 ],\n",
       "       [0.96491498]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323.97860860824585\n",
      "36.13456892967224\n"
     ]
    }
   ],
   "source": [
    "print(model_end_time - model_start_time)\n",
    "print(pred_end_time - pred_start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
